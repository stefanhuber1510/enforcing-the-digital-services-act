{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "openai.api_key = \"\"\n",
    "df = pd.read_pickle(\"./data/all_posts_US_EN.pkl\")\n",
    "#df = pd.read_pickle(\"../ct_df_posts_single_preds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df[\"country\"]==\"US\"]\n",
    "df=df[df[\"language\"]==\"en\"]\n",
    "df = df[df[\"caption\"]!=\"\"]\n",
    "#df.to_pickle(\"data/all_posts_US_EN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(\"username\").count().sort_values(ascending=False,by=\"caption\").iloc[:-10]\n",
    "author_sample = list(df2.sample(20,random_state=1).index)\n",
    "del df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"username\"].isin(author_sample)]\n",
    "df = df.groupby(\"username\").apply(lambda x: x.sample(n=50,random_state=1, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 1 minute per author for 50 posts\n",
    "df2 = pd.DataFrame ([\"author\"]+[\"text\"]+[99]*1536).transpose()\n",
    "for username in df.index.get_level_values(\"username\").unique():\n",
    "    i=0\n",
    "    for caption in df.loc[username][\"caption\"]:\n",
    "        embedding = openai.Embedding.create(input = [caption],\n",
    "                    model=\"text-embedding-ada-002\")['data'][0]['embedding']\n",
    "        df2.loc[len(df2)] = [f\"{username}{i}\"]+[caption]+embedding\n",
    "        i+=1\n",
    "    # just to ensure that it isn't stuck\n",
    "    if len(df2)%100==1:print(f\"User {username} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"5k-embeddings.pkl\",\"wb\") as f:\n",
    "    pickle.dump(df2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/5k-embeddings.pkl\",\"rb\") as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=75, random_state=42)\n",
    "umap_embeddings = reducer.fit_transform(df.iloc[1:4001,2:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 20 seconds, but nonsense?\n",
    "topic_model = BERTopic(min_topic_size=50)\n",
    "#umap_embeddings = topic_model._reduce_dimensionality(df2.iloc[1:4001,2:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>381</td>\n",
       "      <td>-1_this_the_to_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "      <td>0_by_the_de_with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>462</td>\n",
       "      <td>1_the_to_this_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>368</td>\n",
       "      <td>2_and_to_the_you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>235</td>\n",
       "      <td>3_in_the_beach_this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>233</td>\n",
       "      <td>4_the_and_birthday_you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>5_the_of_to_for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>192</td>\n",
       "      <td>6_legend_prince_man_booty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>190</td>\n",
       "      <td>7_my_the_you_outfit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>137</td>\n",
       "      <td>8_pepe_photo_with_lospicantes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>131</td>\n",
       "      <td>9_weekend_morning_happy_friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>125</td>\n",
       "      <td>10_yoga_to_practice_and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>123</td>\n",
       "      <td>11_dogsrule_worldwideunionoftroublemakers_dogs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12_miami_london_la_to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>106</td>\n",
       "      <td>13_devourpower_tag_nyc_devourpowertv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>90</td>\n",
       "      <td>14_my_video_youtube_link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>89</td>\n",
       "      <td>15_swipe_ig_stories_you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>79</td>\n",
       "      <td>16_chocolate_pbexplores_in_ha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>59</td>\n",
       "      <td>17_you_and_to_that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>59</td>\n",
       "      <td>18_beyourselftofreeyourself_phatfashionicon_el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>54</td>\n",
       "      <td>19_hair_my_fresh_you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1    381                                  -1_this_the_to_of\n",
       "1       0    579                                   0_by_the_de_with\n",
       "2       1    462                                   1_the_to_this_of\n",
       "3       2    368                                   2_and_to_the_you\n",
       "4       3    235                                3_in_the_beach_this\n",
       "5       4    233                             4_the_and_birthday_you\n",
       "6       5    198                                    5_the_of_to_for\n",
       "7       6    192                          6_legend_prince_man_booty\n",
       "8       7    190                                7_my_the_you_outfit\n",
       "9       8    137                      8_pepe_photo_with_lospicantes\n",
       "10      9    131                     9_weekend_morning_happy_friday\n",
       "11     10    125                            10_yoga_to_practice_and\n",
       "12     11    123  11_dogsrule_worldwideunionoftroublemakers_dogs...\n",
       "13     12    110                              12_miami_london_la_to\n",
       "14     13    106               13_devourpower_tag_nyc_devourpowertv\n",
       "15     14     90                           14_my_video_youtube_link\n",
       "16     15     89                            15_swipe_ig_stories_you\n",
       "17     16     79                      16_chocolate_pbexplores_in_ha\n",
       "18     17     59                                 17_you_and_to_that\n",
       "19     18     59  18_beyourselftofreeyourself_phatfashionicon_el...\n",
       "20     19     54                               19_hair_my_fresh_you"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_embeddings.shape\n",
    "topic_model.fit(df.iloc[1:4001,1], embeddings=umap_embeddings)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3905</td>\n",
       "      <td>0_the_to_and_in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>1_orca_koko_keegs_karl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                    Name\n",
       "0      0   3905         0_the_to_and_in\n",
       "1      1    102  1_orca_koko_keegs_karl"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model = BERTopic(min_topic_size=50)\n",
    "topic_model.fit_transform(df[1])\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1606</td>\n",
       "      <td>-1_the_with_to_and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>0_yoga_practice_to_and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>1_yea_need_yup_me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "      <td>2_devourpower_tag_nyc_devourpowertv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>3_youaretheentireuniverse_race_2019_heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>64_gbfmovie_misterwillett_tribeca_xoshroq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>65_09_urban_teamnewfriends_preschool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>66_qls_questlovesupreme_about_music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>67</td>\n",
       "      <td>10</td>\n",
       "      <td>67_dirty_bathroom_room_dirtysouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>68_prettylittleliars_pll_alison_you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                        Name\n",
       "0      -1   1606                          -1_the_with_to_and\n",
       "1       0    188                      0_yoga_practice_to_and\n",
       "2       1    187                           1_yea_need_yup_me\n",
       "3       2    106         2_devourpower_tag_nyc_devourpowertv\n",
       "4       3     99  3_youaretheentireuniverse_race_2019_heaven\n",
       "..    ...    ...                                         ...\n",
       "65     64     11   64_gbfmovie_misterwillett_tribeca_xoshroq\n",
       "66     65     10        65_09_urban_teamnewfriends_preschool\n",
       "67     66     10         66_qls_questlovesupreme_about_music\n",
       "68     67     10           67_dirty_bathroom_room_dirtysouth\n",
       "69     68     10         68_prettylittleliars_pll_alison_you\n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "topic_model.fit(df2.iloc[1:4001,1], embeddings=np.array(df2.iloc[1:4001,2:]))\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(min_topic_size=10)\n",
    "topics, probs = topic_model.fit_transform(df[1].iloc[1:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same sample with less topics\n",
    "# concatenate the posts to find categories of influencers\n",
    "# consider simple clustering with GPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
