{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I intend to import the annotated data and establish a baseline performance of the non-fine-tuned davinci-instruct model <br>\n",
    "prerequistes:\n",
    "- A wroking prompt\n",
    "- PoC that davinci works better than the other models<br><br>\n",
    "Steps after:\n",
    "- finetune model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next up: now that I've got a reasonable prompt for the large models, lets compare against the small ones and write up a report for thales - by tonight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import openai\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import json\n",
    "\n",
    "# import large file\n",
    "with open('annotated_sample.pkl', 'rb') as f:\n",
    "    pre_df = pkl.load(f)\n",
    "\n",
    "# shorten to not murder my computer\n",
    "# df = pre_df.iloc[:50]\n",
    "#del pre_df\n",
    "\n",
    "# retrieve API key\n",
    "import os\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "# print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute for small experiments\n",
    "\n",
    "# sample 30 ads, 30 non-ads\n",
    "df = pd.concat([pre_df[pre_df.loc[:,\"label\"]==True].sample(30),\n",
    "                pre_df[pre_df.loc[:,\"label\"]==False].sample(30)]).copy()\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "del pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter at 10\n",
      "Counter at 20\n",
      "sleep at 24\n",
      "Counter at 30\n",
      "Counter at 40\n",
      "sleep at 49\n",
      "Counter at 50\n",
      "Counter at 60\n",
      "Counter at 70\n",
      "sleep at 74\n",
      "Counter at 80\n",
      "Counter at 90\n",
      "sleep at 99\n",
      "Counter at 100\n",
      "Counter at 110\n",
      "Counter at 120\n",
      "sleep at 124\n",
      "Counter at 130\n",
      "Counter at 140\n",
      "sleep at 149\n",
      "Counter at 150\n",
      "Counter at 160\n",
      "Counter at 170\n",
      "sleep at 174\n",
      "Counter at 180\n",
      "Counter at 190\n",
      "sleep at 199\n",
      "Counter at 200\n",
      "Counter at 210\n",
      "Counter at 220\n",
      "sleep at 224\n",
      "Counter at 230\n",
      "Counter at 240\n",
      "sleep at 249\n",
      "Counter at 250\n",
      "Counter at 260\n",
      "Counter at 270\n",
      "sleep at 274\n",
      "Counter at 280\n",
      "Counter at 290\n",
      "sleep at 299\n",
      "Counter at 300\n",
      "Counter at 310\n",
      "Counter at 320\n",
      "sleep at 324\n",
      "Counter at 330\n",
      "Counter at 340\n",
      "sleep at 349\n",
      "Counter at 350\n",
      "Counter at 360\n",
      "Counter at 370\n",
      "sleep at 374\n",
      "Counter at 380\n",
      "Counter at 390\n",
      "sleep at 399\n",
      "Counter at 400\n",
      "Counter at 410\n",
      "Counter at 420\n",
      "sleep at 424\n",
      "Counter at 430\n",
      "Counter at 440\n",
      "sleep at 449\n",
      "Counter at 450\n",
      "Counter at 460\n",
      "Counter at 470\n",
      "sleep at 474\n",
      "Counter at 480\n",
      "Counter at 490\n",
      "sleep at 499\n",
      "Counter at 500\n",
      "Counter at 510\n",
      "Counter at 520\n",
      "sleep at 524\n",
      "Counter at 530\n",
      "Counter at 540\n",
      "sleep at 549\n",
      "Counter at 550\n",
      "Counter at 560\n",
      "Counter at 570\n",
      "sleep at 574\n",
      "Counter at 580\n",
      "Counter at 590\n",
      "sleep at 599\n",
      "Counter at 600\n",
      "Counter at 610\n",
      "Counter at 620\n",
      "sleep at 624\n",
      "Counter at 630\n",
      "Counter at 640\n",
      "sleep at 649\n",
      "Counter at 650\n",
      "Counter at 660\n",
      "Counter at 670\n",
      "sleep at 674\n",
      "Counter at 680\n",
      "Counter at 690\n",
      "sleep at 699\n",
      "Counter at 700\n",
      "Counter at 710\n",
      "Counter at 720\n",
      "sleep at 724\n",
      "Counter at 730\n",
      "Counter at 740\n",
      "sleep at 749\n",
      "Counter at 750\n",
      "Counter at 760\n",
      "Counter at 770\n",
      "sleep at 774\n",
      "Counter at 780\n",
      "Counter at 790\n",
      "sleep at 799\n",
      "Counter at 800\n",
      "Counter at 810\n",
      "Counter at 820\n",
      "sleep at 824\n",
      "Counter at 830\n",
      "Counter at 840\n",
      "sleep at 849\n",
      "Counter at 850\n",
      "Counter at 860\n",
      "Counter at 870\n",
      "sleep at 874\n",
      "Counter at 880\n",
      "Counter at 890\n",
      "sleep at 899\n",
      "Counter at 900\n",
      "Counter at 910\n",
      "Counter at 920\n",
      "sleep at 924\n",
      "Counter at 930\n",
      "Counter at 940\n",
      "sleep at 949\n",
      "Counter at 950\n",
      "Counter at 960\n",
      "Counter at 970\n",
      "sleep at 974\n",
      "Counter at 980\n",
      "Counter at 990\n",
      "sleep at 999\n",
      "Counter at 1000\n",
      "Counter at 1010\n",
      "Counter at 1020\n",
      "sleep at 1024\n",
      "Counter at 1030\n",
      "Counter at 1040\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-text-davinci-003 in organization org-cq5XYRxHaY8l2D7xud69qR86 on requests per min. Limit: 60.000000 / min. Current: 70.000000 / min. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 31\u001b[0m\n\u001b[1;32m     16\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMight the following post have been sponsored? \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m Post: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mAll my teenage girl dreams come true tonight! Thank you so much @sherrihill for having me perform at New York Fashion week! üñ§#SherriHillNYFW\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Potentially Sponsored (True/False): True\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m Post: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mLife imitating #art üé®üé®\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mlive your #fantasy ‚ú®‚ú®\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mStrolling in #beverlyhills #losangeles \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m#Illustration by @donna_adi üé®üé®\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mHappy to be back In #usa #la\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Potentially Sponsored (True/False): False\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m Post:\u001b[39m\u001b[39m{\u001b[39;00mtxt\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m Potentially Sponsored (True/False): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39m# Generate a response from openai.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# Flexibly choose ada/babbage/curie/davinci as engine. For davinci, use text-davinci-002.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#prompt=(\"warning\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m#else:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#    print(\"warning\")\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     32\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     34\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     36\u001b[0m     stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     37\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[39m# Save the response text\u001b[39;00m\n\u001b[1;32m     41\u001b[0m completions[\u001b[39mstr\u001b[39m(x)]\u001b[39m.\u001b[39mappend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         ),\n\u001b[1;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_requestor.py:679\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    677\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 679\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    680\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    681\u001b[0m     )\n\u001b[1;32m    682\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-text-davinci-003 in organization org-cq5XYRxHaY8l2D7xud69qR86 on requests per min. Limit: 60.000000 / min. Current: 70.000000 / min. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
     ]
    }
   ],
   "source": [
    "# Generate classifications for davinci only\n",
    "\n",
    "completions ={\"0\":[],\"1\":[]} #{\"text-ada-001\":[], \"text-babbage-001\":[], \"text-curie-001\":[], \"text-davinci-002\":[]}\n",
    "i=0\n",
    "for x in range(1):#,\"text-ada-001\", \"text-babbage-001\", \"text-curie-001\"]:\n",
    "    #i=0\n",
    "    for txt in df.loc[:,\"caption\"]:\n",
    "        i+=1\n",
    "        if i%10==0: print(f\"Counter at {i}\")\n",
    "\n",
    "        # timer to stay within my quota\n",
    "        if i%25==24:\n",
    "            print(f\"sleep at {i}\") \n",
    "            time.sleep(65)\n",
    "        \n",
    "        prompt = f\"Might the following post have been sponsored? \\n\\n Post: 'All my teenage girl dreams come true tonight! Thank you so much @sherrihill for having me perform at New York Fashion week! üñ§#SherriHillNYFW'\\n Potentially Sponsored (True/False): True\\n\\n Post: 'Life imitating #art üé®üé®\\nlive your #fantasy ‚ú®‚ú®\\nStrolling in #beverlyhills #losangeles \\n#Illustration by @donna_adi üé®üé®\\nHappy to be back In #usa #la'\\n Potentially Sponsored (True/False): False\\n\\n Post:{txt}\\n Potentially Sponsored (True/False): \"\n",
    "\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=3,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        # Save the response text\n",
    "        completions[str(x)].append(response[\"choices\"][0][\"text\"])\n",
    "    print(str(x),\" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[216 133]\n",
      " [ 33 664]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions to booleans\n",
    "completions_as_boolean = {}\n",
    "for model in [\"0\"]:\n",
    "    completions_as_boolean[model] = [True if response.__contains__(\"rue\") \n",
    "                          else False if response.__contains__(\"als\")\n",
    "                          else \"warning\" for response in completions[model]]\n",
    "\n",
    "    # check for bad completions\n",
    "    if \"warning\" in completions[model]: print(\"Warning\")\n",
    "\n",
    "    # obtain confusion matrix\n",
    "\n",
    "    print(confusion_matrix(df.iloc[:1046,39],completions_as_boolean[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add predictions to df\n",
    "df_baseline_predictions = df.iloc[:1046].copy()\n",
    "df_baseline_predictions.reset_index(drop=False, inplace=True)\n",
    "df_baseline_predictions[\"baseline\"] = completions_as_boolean[\"0\"]\n",
    "\n",
    "# save\n",
    "with open('results.pkl', 'wb') as file:\n",
    "    pkl.dump(df_baseline_predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.62      0.72       349\n",
      "        True       0.83      0.95      0.89       697\n",
      "\n",
      "    accuracy                           0.84      1046\n",
      "   macro avg       0.85      0.79      0.81      1046\n",
      "weighted avg       0.84      0.84      0.83      1046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_baseline_predictions[\"label\"],df_baseline_predictions[\"baseline\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models (tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classifications for all models\n",
    "\n",
    "completions = {\"text-ada-001\":[], \"text-babbage-001\":[], \"text-curie-001\":[], \"text-davinci-002\":[]}\n",
    "for model in [\"text-davinci-003\",\"text-ada-001\", \"text-babbage-001\", \"text-curie-001\"]:\n",
    "    i=0\n",
    "    for txt in df.loc[:,\"caption\"]:\n",
    "        i+=1\n",
    "        if i%10==0: print(f\"Counter at {i}\")\n",
    "\n",
    "        # timer to stay within my quota\n",
    "        if i==24: time.sleep(65)\n",
    "        if i==49: time.sleep(65)\n",
    "        if i==74: time.sleep(65)\n",
    "        if i==99: time.sleep(65)\n",
    "\n",
    "        prompt1 = f\"Judge whether it is likely that the following caption comes from a post that has been sponsored. If you are very uncertain err rather towards judging 'False'.\\n\\n Post: 'aaaa meu look de hoje √© da @cea_brasil ‚ù§Ô∏è patrocinadora e dona do look oficial do Rock in Rio! Aproveitem pra acompanhar o perfil da C&A pra ver todos os lookinhos maravilhosos! Amo muito! ‚ù§Ô∏è'\\n Sponsored (True/False): True\\n\\n Post: 'side by side ~ sisters ‚ù§Ô∏è'\\n Sponsored (True/False): True\\n\\n Post:{txt}\\n Sponsored (True/False):\"\n",
    "        \n",
    "        prompt2 = f\"Judge whether it is likely that the following caption comes from a post that has been sponsored. If you are very uncertain err rather towards judging 'True'.\\n\\n Post: 'aaaa meu look de hoje √© da @cea_brasil ‚ù§Ô∏è patrocinadora e dona do look oficial do Rock in Rio! Aproveitem pra acompanhar o perfil da C&A pra ver todos os lookinhos maravilhosos! Amo muito! ‚ù§Ô∏è'\\n Sponsored (True/False): True\\n\\n Post: 'side by side ~ sisters ‚ù§Ô∏è'\\n Sponsored (True/False): True\\n\\n Post:{txt}\\n Sponsored (True/False):\"\n",
    "\n",
    "        # Generate a response from openai.\n",
    "        # Flexibly choose ada/babbage/curie/davinci as engine. For davinci, use text-davinci-002.\n",
    "        prompt=(\"warning\")\n",
    "        if model in [\"text-ada-001\", \"text-babbage-001\"]:\n",
    "            if i==1: print(\"pompt1\")\n",
    "            prompt=prompt1\n",
    "        elif model in [\"text-curie-001\", \"text-davinci-002\"]:\n",
    "            if i==1: print(\"pompt2\")\n",
    "            prompt=prompt2\n",
    "        else:\n",
    "            print(\"warning\")\n",
    "        response = openai.Completion.create(\n",
    "            engine=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=5,\n",
    "            n=3,\n",
    "            stop=None,\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        # Save the response text\n",
    "        completions[model].append(response[\"choices\"][0][\"text\"])\n",
    "    print(model+\" done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
